{\rtf1\ansi\ansicpg1252\cocoartf2578
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fmodern\fcharset0 Courier-Bold;\f1\fmodern\fcharset0 Courier;}
{\colortbl;\red255\green255\blue255;\red202\green202\blue202;\red23\green23\blue23;\red113\green184\blue255;
\red89\green156\blue62;}
{\*\expandedcolortbl;;\cssrgb\c83137\c83137\c83137;\cssrgb\c11765\c11765\c11765;\cssrgb\c50980\c77647\c100000;
\cssrgb\c41569\c66275\c30980;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sl380\partightenfactor0

\f0\b\fs28 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 **Write-Up - Contrastive Principal Component Analysis**
\f1\b0 \cb1 \
\
\
\cf4 \cb3 \strokec4 ---\cf2 \cb1 \strokec2 \
\
\
\cb3 Contrastive PCA can generally be used on datasets in which the desired features of interest are typically nuanced or are overshadowed by other dominant, highly variant, and usually generic directions, mostly to the fact that highest variance feature corresponds to the most interesting feature.\cb1 \
\
\
\cf4 \cb3 \strokec4 ---\cf2 \cb1 \strokec2 \
\
\
\cb3 Dimensionality reduction techniques like PCA are used on datasets in order to compress the representation of the data and to assist the preprocessing stage of the data, before feeding it to an ML/AI model. In this stage, it always preferred to optimised the data to make it more suitable for the next stage of ML analysis. The expected outcome of dimensionality reduction is not only compression of data, but also to obtain a better understanding of the data as such, as such techniques are usually used in unsupervised learning models.\cb1 \
\
\cb3 In most datasets, the highest varying directions may not correspond to the desired directions of interest, interest they may correspond to non-interesting directions like the follows:\cb1 \
\
\cf4 \cb3 \strokec4 * \cf2 \strokec2   Arfticats (digital products used in the AI model - input, output, intermediate results, etc.)\cb1 \
\cf4 \cb3 \strokec4 * \cf2 \strokec2   Demographic variations\cb1 \
\cf4 \cb3 \strokec4 * \cf2 \strokec2   Correlated measurement techniques (sensors)\cb1 \
\
\cb3 For instance, in biology, the dominant PCs (principal components = highest varying directions) almost always correlated with artifacts.\cb1 \
\
\
\cf4 \cb3 \strokec4 ---\cf2 \cb1 \strokec2 \
\
\cb3 PCA aims to identify dominant trends in a datset, but dataset specific patterns (which might not be dominant), are caught by cPCA and may be missed by PCA. Also, cPCA automatically identifies the most projections that exhibit the most interesting variables across datasets. The fundamental idea is to search for components in which the target dataset has high variance but the background dataset has low variance.\cb1 \
\
\cb3 When there are multiple datasets, and there are sources of variation that are "enchanced" or "enriched" in one dataset when compared to the other dataset, Contrastive PCA would be the appropriate dimensionality-reduction technique here, as opposed to the PCA and the others.\cb1 \
\
\
\cf4 \cb3 \strokec4 ---\cf2 \cb1 \strokec2 \
\
\
\cb3 The Contrastive PCA technique, to put it simply, draws an extension to the regular PCA technique, by involving an additional "background" dataset apart from the existing "target" dataset. Here, the target dataset would be the one enriched with certain interesting directions/features relative to the background dataset. But both target and background datasets will contain the default set of inherent directions with inherent variations. Here, as we need to bring out the enriched sources of variations to the front, we need to give the technique another point of reference, which is the background dataset. So the background datset would help in cancelling out commonalities between the target and background, thereby leaving behind the desired features of interest in the target dataset. This distinction can then then be plotted into clusters which would help us get a better interpretation of the target dataset.\cb1 \
\
\
\cf4 \cb3 \strokec4 ---\cf2 \cb1 \strokec2 \
\
\cb3 cPCA should not be used to to discriminate between two datasets, it is merely a tool for assistance in unsupervised learning scenarios, in order to better understand the dataset better. It cannot be used, for example, in supervised learning scenarios to narrow down on the best classifier for the datasets.\cb1 \
\
\
\cf4 \cb3 \strokec4 ---\cf2 \cb1 \strokec2 \
\
\
\cb3 Different types of datasets and contexts where Contrastive PCA would be applicable:\cb1 \
\
\cf4 \cb3 \strokec4 * \cf2 \strokec2   Datasets which are collected in a time series - Before vs After problems\cb1 \
\
\cf5 \cb3 \strokec5 >\cf2 \strokec2  Here, the datasets are collected for the same variables but over different periods of time, so in this case all generic and background factors would remain constant, and the desired sources of variations would be the ones recorded over time, and they will be outputted when cPCA is done on a recent dataset with another dataset from the past.\cb1 \
\
\
\cf4 \cb3 \strokec4 * \cf2 \strokec2   Diseased vs Healthy (Biology) or Experimental vs Control\cb1 \
\
\cf5 \cb3 \strokec5 >\cf2 \strokec2  Any scenario where the usual structure of an entity goes through a deviation due to external factors, keeping the surrounding factors constant, cPCA would be an appropriate technique to use. For example, if there are two groups from the same neighborhood, one diseased/experimental and the other healthy/controlled.\cb1 \
\
\cf4 \cb3 \strokec4 * \cf2 \strokec2   Heterogenous vs Single - Inter vs Intra\cb1 \
\
\cf5 \cb3 \strokec5 >\cf2 \strokec2  In many cases, when we use two vast datasets with multiple factors variables recorded over a lot of entities, there will definitely be "mulltiple layers of variations". When one such dataset is compared, the variations that might dominate are the ones on the top layer. But if we need to extrac the features from a layer somewhere below, we would need to bring in another point of reference which would have everything in common with the top layer. So we bring in another such vast dataset. In most cases, the top layers of such datasets will be common, hence we cPCA is done on them, what gets left behind are the details and sources of variations from the middle and bottom layers, which are specific to a certain dataset. For example, when comparing two countries from the same continent, lot of upper layer features and variations might be similar, whereas it is a fact that each of the two countries will have unique features of interest within them too, and these are what we would be able to extract when cPCA is done on these two datasets.\cb1 \
\
\cf4 \cb3 \strokec4 * \cf2 \strokec2   Superimposed Images\cb1 \
\
\cf5 \cb3 \strokec5 >\cf2 \strokec2  In a scenario where there is a dataset with thousands of images of the same category (constant scenery, faces, etc) and a good portion (say half) of the images are superimposed by another set of figures (maybe small yet distinct), it becomes hard for PCA to analys the dataseted and produce useful clusters, epecially when the images are in grayscale. In this case, we would have generate a background dataset with the portion of the dataset that aren't superimposed, and since the covariant features of all images in the dataset (regardless of superimposition) would be very close, cPCA would work in cancelling out the common features and highlighting the superimposition, thereby helping in understanding the target dataset better.\cb1 \
\
\cf4 \cb3 \strokec4 * \cf2 \strokec2   Applications where relevant data from a second dataset can be used to finetune the target dataset in order to interpret additional details that might otherwise be ignored because of its insignficance - Can help in increasing the precision of analysis and extraction of trivial (yet maybe important) sources of variation\cb1 \
\
\
\cf4 \cb3 \strokec4 ---\cf2 \cb1 \strokec2 \
\
\
\
}